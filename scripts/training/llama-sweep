#!/bin/bash
set -e
PORT=$RANDOM
PORT=$((PORT + 1000))

BS=$4
LR=$5
if [ -z "$BS" ]; then
    BS=32
fi
if [ -z "$LR" ]; then
    LR=2e-4
fi
CONFIG=$1
EF=$2
K=$3

# OPTIM_ARGS="--optimizer muon --lr $LR --lr_warmup_steps 50 --batch_size=$BS"
OPTIM_ARGS="--optimizer adam --lr $LR --lr_warmup_steps 50 --batch_size=$BS"
AFFINE="--post_encoder_scale=True --train_post_encoder=True"
SKIP="--skip_connection=True"
NAME=
LLAMA_IN_HOOKPOINTS="layers.0.post_attention_layernorm layers.1.post_attention_layernorm layers.2.post_attention_layernorm layers.3.post_attention_layernorm layers.4.post_attention_layernorm layers.5.post_attention_layernorm layers.6.post_attention_layernorm layers.7.post_attention_layernorm layers.8.post_attention_layernorm layers.9.post_attention_layernorm layers.10.post_attention_layernorm layers.11.post_attention_layernorm layers.12.post_attention_layernorm layers.13.post_attention_layernorm layers.14.post_attention_layernorm layers.15.post_attention_layernorm"
IN_HOOKPOINTS=
if [ -z "$CONFIG" ]; then
    CONFIG=tied
fi
if [ "$CONFIG" == "none" ]; then
    CROSSING=""
elif [ "$CONFIG" == "tied" ]; then
    CROSSING="--cross_layer=16 --coalesce_topk=concat --topk_coalesced=False"
elif [ "$CONFIG" == "tied-pre" ]; then
    CROSSING="--cross_layer=16 --coalesce_topk=concat --topk_coalesced=False"
    IN_HOOKPOINTS="--hookpoints_in $LLAMA_IN_HOOKPOINTS"
elif [ "$CONFIG" == "cross" ]; then
    CROSSING="--cross_layer=4"
elif [ "$CONFIG" == "cross16" ]; then
    CROSSING="--cross_layer=16"
elif [ "$CONFIG" == "no-skip" ]; then
    SKIP="" CROSSING=""
else
    echo "Invalid option: $CONFIG"
    exit 1
fi
NAME=${CONFIG}

if [ -z "$EF" ]; then
    EF=64
fi
NAME=${NAME}_ef${EF}
if [ -z "$K" ]; then
    K=16
fi
NAME=${NAME}_k${K}

NAME=${NAME}_bs${BS}_lr${LR}

SAVE_ARGS="--save_every 100000000000"
if [ -z "$DONT_SAVE" ]; then
    SAVE_ARGS=""
fi

N_DEVICE_CHARS=${#CUDA_VISIBLE_DEVICES}
N_DEVICES=$(( ($N_DEVICE_CHARS + 1) / 2 ))
echo "Using $N_DEVICES devices"

OPTIMIZER=adam8 \
WANDB_ENTITY=eleutherai uv run torchrun --master_port $PORT --nproc_per_node gpu -m sparsify \
meta-llama/Llama-3.2-1B --ctx_len 128 \
--transcode=True $SKIP \
--expansion_factor=$EF --k=$K --tp=$N_DEVICES \
$IN_HOOKPOINTS \
--hookpoints layers.0.mlp layers.1.mlp layers.2.mlp layers.3.mlp layers.4.mlp layers.5.mlp layers.6.mlp layers.7.mlp layers.8.mlp layers.9.mlp layers.10.mlp layers.11.mlp \
layers.12.mlp layers.13.mlp layers.14.mlp layers.15.mlp \
--filter_bos True \
$CROSSING \
$AFFINE --normalize_io=True \
--run_name llama-sweep/$NAME \
$SAVE_ARGS \
$OPTIM_ARGS \
# --dtype bfloat16
