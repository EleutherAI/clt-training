#!/bin/bash
set -e
PORT=$RANDOM
PORT=$((PORT + 1000))
WANDB_ENTITY=eleutherai uv run torchrun --master_port $PORT --nproc_per_node gpu -m sparsify \
meta-llama/Llama-3.2-1B --ctx_len 128 \
--transcode=True --skip_connection=True \
--batch_size=4 --expansion_factor=64 --tp=4 \
--hookpoints layers.0.mlp layers.1.mlp layers.2.mlp layers.3.mlp layers.4.mlp layers.5.mlp layers.6.mlp layers.7.mlp layers.8.mlp layers.9.mlp layers.10.mlp layers.11.mlp \
layers.12.mlp layers.13.mlp layers.14.mlp layers.15.mlp \
--run_name llama-finetune/$1 ${@:2} \
--optimizer muon --lr 1e-5 \
--filter_bos True \
--cross_layer=16 \
--coalesce_topk=concat --topk_coalesced=False --post_encoder_scale=True --normalize_io=True \
--finetune ../e2e/checkpoints/llama/anneal-adam \
--loss_fn kl
# --finetune mntss/skip-transcoder-Llama-3.2-1B-131k-nobos
# --finetune checkpoints/llama-finetune/test-0 \
# --k=12
# --loss_fn kl --filter_bos True
