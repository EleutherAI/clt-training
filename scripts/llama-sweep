#!/bin/bash
set -e
PORT=$RANDOM
PORT=$((PORT + 1000))

BS=$1
LR=$2
# OPTIM_ARGS="--optimizer muon --lr $LR --lr_warmup_steps 50 --batch_size=$BS"
OPTIM_ARGS="--optimizer adam --lr $LR --lr_warmup_steps 50 --batch_size=$BS"
NAME=bs${BS}_lr${LR}
if [ -z "$3" ]; then
    CROSSING="--cross_layer=16"
else
    if [ "$3" == "none" ]; then
        CROSSING=""
    elif [ "$3" == "tied" ]; then
        CROSSING="--cross_layer=16 --coalesce_topk=concat --topk_coalesced=False"
    elif [ "$3" == "cross" ]; then
        CROSSING="--cross_layer=4"
    elif [ "$3" == "cross16" ]; then
        CROSSING="--cross_layer=16"
    else
        echo "Invalid option: $3"
        exit 1
    fi
    NAME=${NAME}_${3}
fi
if [ -z "$4" ]; then
    EF=64
else
    EF=$4
fi
NAME=${NAME}_ef${EF}
if [ -z "$5" ]; then
    K=16
else
    K=$5
fi
NAME=${NAME}_k${K}

SAVE_ARGS="--save_every 100000000000"
if [ -z "$DO_SAVE" ]; then
    SAVE_ARGS=""
fi

WANDB_ENTITY=eleutherai uv run torchrun --master_port $PORT --nproc_per_node gpu -m sparsify \
meta-llama/Llama-3.2-1B --ctx_len 128 \
--transcode=True --skip_connection=True \
--expansion_factor=$EF --k=$K --tp=4 \
--hookpoints layers.0.mlp layers.1.mlp layers.2.mlp layers.3.mlp layers.4.mlp layers.5.mlp layers.6.mlp layers.7.mlp layers.8.mlp layers.9.mlp layers.10.mlp layers.11.mlp \
layers.12.mlp layers.13.mlp layers.14.mlp layers.15.mlp \
--filter_bos True \
$CROSSING \
--post_encoder_scale=True --normalize_io=True \
--run_name llama-sweep/$NAME \
$SAVE_ARGS \
$OPTIM_ARGS \
