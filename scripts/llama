#!/bin/bash
set -e
PORT=$RANDOM
PORT=$((PORT + 1000))
WANDB_ENTITY=eleutherai uv run torchrun --master_port $PORT --nproc_per_node gpu -m sparsify \
meta-llama/Llama-3.2-1B --ctx_len 128 \
--transcode=True --skip_connection=True \
--batch_size=8 --expansion_factor=64 --tp=4 \
--hookpoints layers.0.mlp layers.1.mlp layers.2.mlp layers.3.mlp layers.4.mlp layers.5.mlp layers.6.mlp layers.7.mlp layers.8.mlp layers.9.mlp layers.10.mlp layers.11.mlp \
layers.12.mlp layers.13.mlp layers.14.mlp layers.15.mlp \
--run_name llama/$1 ${@:2} \
--optimizer muon --lr 1e-4 \
--filter_bos True \
--cross_layer=16 \
--coalesce_topk=concat --topk_coalesced=False --post_encoder_scale=True --normalize_io=True \
# --k_decay_steps=5000 --k_anneal_mul=2